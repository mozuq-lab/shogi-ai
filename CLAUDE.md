# 将棋AI開発ガイド（蒸留Transformer）

## プロジェクト概要

水匠5の評価値を教師として蒸留した軽量Transformer評価ネットワークを構築する。
Value Network（局面→評価値）のみを実装対象とし、ポリシーネットは将来の拡張とする。

## 開発環境

| 環境 | 用途 | 備考 |
|------|------|------|
| Mac (Apple Silicon) | 開発、デバッグ、データ生成 | 現在の開発環境 |
| Windows (RTX 4070 Super) | 本格学習 (Phase 3) | CUDA使用 |

データの受け渡しはクラウドストレージを使用する。

## 開発フェーズと進捗

- [x] Phase 0: 環境構築
- [x] Phase 1: 教師データ生成
- [x] Phase 2: モデル設計・実装
- [x] Phase 3: 学習スクリプト実装（本格学習はWindows環境で実施）
- [x] Phase 4: エンジン組み込み（1手読み）
- [ ] Phase 5: モデル改良（詳細は末尾のタスクリストを参照）

## ディレクトリ構成

```
shogi-ai/
├── external/shogi-cli/     # 将棋エンジン (git submodule)
│   ├── suisho5/            # 水匠5
│   │   ├── YaneuraOu-mac   # Mac用エンジン
│   │   ├── YaneuraOu_NNUE_halfKP256-V830Git_AVX2.exe  # Windows用エンジン
│   │   └── eval/nn.bin     # 評価関数
│   └── hao/                # Hao
│       ├── YaneuraOu-mac   # Mac用エンジン
│       ├── YaneuraOu_NNUE_halfKP256-V830Git_AVX2.exe  # Windows用エンジン
│       └── eval/nn.bin     # 評価関数
├── shogi_utils/            # 将棋関連ユーティリティ
│   ├── __init__.py
│   └── usi_engine.py       # USIエンジンラッパー ✓
├── models/                 # モデル定義 ✓
│   ├── __init__.py
│   ├── value_transformer.py  # Transformerモデル
│   ├── sfen_parser.py        # SFENパーサー
│   ├── dataset.py            # PyTorch Dataset
│   └── features.py           # 拡張特徴量（利き、玉距離等）
├── tests/                  # テスト ✓
│   └── test_models.py
├── data/raw/               # 生成データ (.gitignore対象)
├── tools/
│   └── gen_dataset.py      # データ生成スクリプト ✓
├── train/                  # 学習スクリプト ✓
│   └── train.py            # Value Network学習
├── engine/                 # USIエンジン ✓
│   ├── evaluator.py        # モデル評価器
│   └── usi_server.py       # USIプロトコルサーバー
├── scripts/
│   └── usi_test.py         # USI疎通確認スクリプト ✓
├── env/
│   └── README.md           # セットアップ手順
└── reports/                # 評価レポート
```

## 実装済み機能

### USIエンジンラッパー (`shogi_utils/usi_engine.py`)

```python
from shogi_utils import USIEngine, get_engine_path, get_default_engine_path

# 水匠5を使用（デフォルト）
with USIEngine(get_default_engine_path()) as engine:
    engine.init_usi()
    engine.set_option("USI_OwnBook", False)
    engine.is_ready()
    engine.set_position(moves=["7g7f", "3c3d"])
    result = engine.go(depth=10)  # or movetime=500
    print(result.bestmove, result.score_cp)

# Haoを使用
with USIEngine(get_engine_path("hao")) as engine:
    engine.init_usi()
    # ...
```

### データ生成スクリプト (`tools/gen_dataset.py`)

将棋エンジン同士の自己対局で教師データを生成する。

```bash
# 深さ10で1対局（デフォルト、水匠5使用）
python tools/gen_dataset.py -n 1

# 深さ15で10対局
python tools/gen_dataset.py -n 10 --depth 15 -o data/raw/depth15.jsonl

# 時間指定（500ms）で生成
python tools/gen_dataset.py -n 10 --movetime 500

# 並列実行（4ワーカー）
python tools/gen_dataset.py -n 100 --depth 10 --workers 4

# Haoエンジンを使用
python tools/gen_dataset.py -n 10 --engine-type hao
python tools/gen_dataset.py -n 10 --depth 15 --engine-type hao -o data/raw/hao_depth15.jsonl
```

#### エンジン選択

| オプション | エンジン | 説明 |
|-----------|---------|------|
| `--engine-type suisho5` | 水匠5 | デフォルト |
| `--engine-type hao` | Hao | 別の評価関数 |
| `--engine /path/to/engine` | 任意 | パス直接指定 |

#### 出力形式 (JSONL)

```json
{"sfen": "startpos", "score_cp": 37, "ply": 0, "game_id": 0, "result": "white_win"}
{"sfen": "startpos moves 2g2f", "score_cp": -136, "ply": 1, "game_id": 0, "result": "white_win"}
```

| フィールド | 説明 |
|-----------|------|
| `sfen` | 局面（startpos + 指し手列） |
| `score_cp` | 評価値（centipawn、手番側視点） |
| `ply` | 手数 |
| `game_id` | 対局ID |
| `result` | 対局結果 (`black_win`, `white_win`, `draw`) |

## 水匠5の使い方

```bash
cd external/shogi-cli/suisho5
echo -e "usi\nisready\nposition startpos\ngo depth 10\nquit" | ./YaneuraOu-mac
```

### パフォーマンス

| 環境 | スレッド数 | nps |
|------|-----------|-----|
| Mac Apple Silicon | 1 | 約 1.0M |
| Mac Apple Silicon | 8 | 約 4.9M |
| Windows Ryzen 7 9700X (AVX2) | 1 | 約 2.4M |
| Windows Ryzen 7 9700X (AVX2) | 8 | 約 15.7M |
| Windows Ryzen 7 9700X (AVX2) | 16 | 約 21.4M |

※ Windows環境ではAVX2版を使用（AVX512VNNI版はRyzen 9000シリーズ非対応）

### Value Network (`models/`)

```python
from models import ValueTransformer, ShogiValueDataset, collate_fn
from torch.utils.data import DataLoader

# モデル作成
model = ValueTransformer(
    d_model=256,
    n_heads=4,
    n_layers=4,
    ffn_dim=512,
    dropout=0.1,
)
# パラメータ数: 約215万

# データセット読み込み
dataset = ShogiValueDataset("data/raw/hao_trial_500_depth10.jsonl")
loader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)

# 推論
batch = next(iter(loader))
output = model(batch["board"], batch["hand"], batch["turn"])  # (batch, 1), [-1, 1]
```

#### 入力表現

- 81マス（盤面）+ 14トークン（持ち駒）= 95トークン
- 駒種埋め込み（29種: 空、先手駒14種、後手駒14種）
- 持ち駒は駒種埋め込み + 枚数埋め込み
- 手番埋め込みを全トークンに加算

#### 拡張特徴量（オプション）

`--use-features` フラグで有効化。各マスに6次元の追加特徴量を付与：

| 特徴量 | 次元 | 説明 |
|--------|------|------|
| attack_map | 2 | 先手/後手の利き（0/1） |
| king_distance | 2 | 先手玉/後手玉からのチェビシェフ距離 |
| piece_value | 1 | 駒価値（先手+、後手-） |
| control | 1 | 支配度（利きの差をtanh正規化） |

```python
# 拡張特徴量を使用したモデル
model = ValueTransformer(use_features=True)

# 拡張特徴量を使用した学習
PYTHONPATH=. python train/train.py --data data.jsonl --use-features
```

### 勝敗補助損失

モデルは評価値に加えて勝率予測も出力。学習時に補助損失として使用：

```bash
# 補助損失の重みを調整（デフォルト: 0.1）
PYTHONPATH=. python train/train.py --data data.jsonl --aux-loss-weight 0.2
```

損失 = MSE(評価値) + weight × BCE(勝率)

### 全ての改良を有効にした学習

```bash
PYTHONPATH=. python train/train.py \
    --data data/raw/large_dataset.jsonl \
    --use-features \
    --aux-loss-weight 0.1 \
    --normalize-turn \
    --augment-flip \
    --cp-noise 7.5 \
    --cp-filter-threshold 1500 \
    --num-workers 4 \
    --epochs 100 \
    --batch-size 512
```

#### 出力

- スカラー値 [-1, 1]（勝率近似、tanh正規化）

#### 評価値の正規化

```python
from models import normalize_cp, denormalize_cp

# centipawn → [-1, 1]
value = normalize_cp(500)   # → 0.395

# [-1, 1] → centipawn
cp = denormalize_cp(0.395)  # → 500
```

## Phase 3: 学習

### 学習スクリプト (`train/train.py`)

```bash
# Mac環境での動作確認（小規模）
PYTHONPATH=. python train/train.py \
    --data data/raw/hao_trial_500_depth10.jsonl \
    --epochs 5 --batch-size 64 --device auto

# Windows環境での本格学習
PYTHONPATH=. python train/train.py \
    --data data/raw/large_dataset.jsonl \
    --epochs 100 --batch-size 512 --device cuda

# 学習再開
PYTHONPATH=. python train/train.py \
    --data data/raw/large_dataset.jsonl \
    --resume checkpoints/epoch_0050.pt
```

#### 主なオプション

| オプション | デフォルト | 説明 |
|-----------|-----------|------|
| `--data` | 必須 | データファイルパス |
| `--epochs` | 100 | エポック数 |
| `--batch-size` | 512 | バッチサイズ |
| `--lr` | 3e-4 | 学習率 |
| `--device` | auto | デバイス（auto/cuda/mps/cpu） |
| `--output-dir` | checkpoints | 出力ディレクトリ |
| `--resume` | - | 再開するチェックポイント |
| `--val-split` | 0.1 | 検証データ割合 |
| `--use-features` | - | 拡張特徴量を使用 |
| `--aux-loss-weight` | 0.1 | 勝敗補助損失の重み |

#### 出力ファイル

```text
checkpoints/
├── best.pt           # ベストモデル（最小val_loss）
├── epoch_XXXX.pt     # 定期保存（--save-every間隔）
├── final.pt          # 最終モデル
└── log_YYYYMMDD_HHMMSS.json  # 学習ログ
```

### ハイパーパラメータ

- データセット: 10万〜100万局面
- 訓練:バリデーション = 9:1
- Optimizer: AdamW
- 学習率: 3e-4（5エポックwarmup後、コサインアニーリング）
- バッチサイズ: 512〜1024
- 損失関数: MSE

## Phase 4: USIエンジン

### USIエンジンの起動

```bash
# ラッパースクリプトで起動（将棋GUIから登録する場合はこちら）
./shogi-ai-engine.sh

# 直接起動
PYTHONPATH=. python engine/usi_server.py --model checkpoints/best.pt

# デバイス指定
PYTHONPATH=. python engine/usi_server.py --model checkpoints/best.pt --device cpu
```

#### 将棋GUIでの使用

将棋所やShogiGUIなどのUSI対応GUIで `shogi-ai-engine.sh` をエンジンとして登録すると対局可能。

#### 対応USIコマンド

| コマンド | 説明 |
|---------|------|
| `usi` | エンジン情報を返す |
| `isready` | モデルを読み込み、`readyok`を返す |
| `position startpos [moves ...]` | 初期局面から指定の手を適用 |
| `position sfen <sfen> [moves ...]` | SFEN局面から指定の手を適用 |
| `go` | 最善手を探索（1手読み） |
| `quit` | 終了 |

#### 評価器 (`engine/evaluator.py`)

```python
from engine.evaluator import Evaluator

# 評価器を初期化
evaluator = Evaluator("checkpoints/best.pt", device="auto")

# SFEN文字列で評価
score = evaluator.evaluate_sfen("startpos moves 7g7f 3c3d")

# python-shogiのBoardで評価
import shogi
board = shogi.Board()
score = evaluator.evaluate_board(board)

# 最善手を探索（1手読み）
best_move, score = evaluator.find_best_move(board)
```

### 依存ライブラリ

- `python-shogi`: 合法手生成に使用

### 制限事項

- 現在は1手読みのみ（各合法手の評価値を比較）
- 探索アルゴリズム（αβ等）は未実装

## コーディング規約

- 型ヒント必須
- docstring: Google style
- テスト: pytest
- Python 3.10+互換（`from __future__ import annotations`使用）

---

## Phase 5: モデル改良タスク

### 優先度A: 即効性が高く低リスク（1-2時間）

- [x] **Gradient Clipping** - 勾配爆発防止
  - `train.py`: `--grad-clip-norm`オプション（デフォルト: 1.0）

- [x] **Label Smoothing** - 勝敗予測の過信防止
  - `train.py`: `--label-smoothing`オプション（デフォルト: 0.05）

- [x] **評価値ノイズ付与** - 過学習抑制
  - `train.py`: `--cp-noise`オプション（デフォルト: 0、推奨: 5-10）

- [x] **簡単局面フィルタ** - 極端な評価値を間引き
  - `train.py`: `--cp-filter-threshold`オプション（推奨: 1500）

### 優先度B: データ効率の最大化（半日〜1日）

- [x] **盤面正規化（手番反転）** - 常に先手視点に統一
  - `train.py`: `--normalize-turn`オプション
  - 効果: 後手番の局面を先手視点に変換し、モデルの学習を効率化

- [x] **左右反転データ拡張** - 対称性を活用
  - `train.py`: `--augment-flip`オプション
  - 効果: データを2倍に（normalize-turnと合わせて使用可能）

- [ ] **重み付き損失（nodes基準）** - 難しい局面を重視
  - `gen_dataset.py`: 探索ノード数をJSONLに保存（SearchResultに既存）
  - `dataset.py`, `train.py`: ノード数が多い局面の損失を重くする

### 優先度C: 学習の安定化・汎化性能向上（半日〜1日）

- [ ] **SWA (Stochastic Weight Averaging)** - 汎化性能向上
  - `train.py`: 末期エポックでSWAを有効化
  - `torch.optim.swa_utils.AveragedModel`使用

- [ ] **EMA (Exponential Moving Average)** - 推論用重み平滑化
  - `train.py`: 学習中にEMA重みを維持、推論時はEMA版を使用

- [ ] **検証セット分割評価** - 弱点の可視化
  - `train.py`: 序盤(ply<30)/中盤(30-80)/終盤(80+)別にval_lossを計算・ログ出力

### 優先度D: 推論改良（半日）

- [ ] **TTA (Test-Time Augmentation)** - 推論精度向上
  - `evaluator.py`: 元盤面と左右反転盤面の評価値を平均

- [ ] **Temperature付きスケーリング** - 出力調整
  - `evaluator.py`: `tanh(x / T)`でチューニングパラメータ導入

### 優先度E: 特徴量拡張（半日〜1日）

- [ ] **王手フラグ追加** - 局面状態の明示
  - `features.py`: python-shogiの`board.is_check()`を活用

- [ ] **駒得スカラー追加** - グローバル特徴
  - `features.py`: 先手の駒価値合計 - 後手の駒価値合計

### 優先度F: モデル構造の実験（数日）

- [ ] **CNN + Transformer** - 局所性を明示
  - `value_transformer.py`: 3×3近傍を扱うCNN埋め込みの上にTransformerを重ねる

- [ ] **RoPE/ALiBi位置埋め込み** - 長手数への汎化
  - `value_transformer.py`: 固定正弦波→Rotary Position Embeddingに変更

### 優先度G: 長期研究課題

- [ ] **マルチタスク（指し手優先度ヘッド）** - ポリシーの粗スーパビジョン
- [ ] **cp→勝率キャリブレーション** - 後処理補正関数
- [ ] **Curriculum Learning** - 序盤→中盤→終盤のデータ比率を段階的に変化
- [ ] **軽量ビーム探索** - αβ等の探索アルゴリズム実装
- [ ] **Endgame特化ヘッド** - 終盤検知時に別ヘッドに切り替え
